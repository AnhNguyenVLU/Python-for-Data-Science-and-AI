{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# như tất cả các bạn đều biết, mọi thứ có thể tăng tốc nếu bạn có GPU NVIDIA\n",
    "#CUDA là khung mà NVIDIA phát triển, cho phép chúng tôi sử dụng GPU để tính toán\n",
    "\n",
    "# PC của tôi là MAC và tôi không có CUDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "Y_train = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 3]), torch.Size([15, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#please tạo tensors từ mảng numpy này\n",
    "# torch.from_numpy (bản sao) hoặc torch.tensor (không phải bản sao!)\n",
    "inputs  = torch.tensor(X_train)\n",
    "targets = torch.tensor(Y_train)\n",
    "\n",
    "#please in hình dạng của những cái căng này\n",
    "# sử dụng .size () hoặc .shape\n",
    "inputs.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đặt tập dữ liệu này lên đầu các đầu vào và mục tiêu của chúng tôi\n",
    "#format: TensorDataset (X, y) trong đó X.shape là (m, n) và y.shape là (m, k)\n",
    "ds = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([91., 88., 64.]), tensor([ 81., 101.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds [1] # đây là một bộ của hai tenxơ, x và y tương ứng\n",
    "# đây LÀ ĐỊNH DẠNG mà pyTorch muốn !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dataloader sẽ tự động tạo một điều tra viên, hãy xem từng lô\n",
    "#means, bạn có thể chỉ cần thực hiện một vòng lặp for trên bộ dữ liệu này\n",
    "# nếu bạn KHÔNG MUỐN sử dụng DataLoader này, vẫn ổn! Nhưng bạn có\n",
    "#để chọn thủ công lô nhỏ (giống như chúng tôi làm trong lớp lô nhỏ LR của mình)\n",
    "from torch.utils.data import DataLoader #this guy is randomized (nếu bạn đặt Shuffle = true)\n",
    "\n",
    "batch_size = 3 # đây là bất kỳ số nào bạn thích\n",
    "# too small thì mã của bạn chạy chậm\n",
    "# too to thì bạn có thể gặp lỗi \"hết bộ nhớ\"\n",
    "\n",
    "dl = DataLoader (ds, batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "#stands cho mạng nơ-ron; mô-đun chứa nhiều lớp có thể\n",
    "# xác định mạng nơ-ron của chúng tôi\n",
    "# chỉ sử dụng một lớp ....\n",
    "# chúng tôi sẽ quay lại đây và thêm một lớp nữa ....\n",
    "#format: nn.Linear (in_features, out_features)\n",
    "#format: nn.Linear (nhiệt độ; lượng mưa; hum, cam; táo)\n",
    "\n",
    "# model = nn.Linear (3, 2)\n",
    "\n",
    "#linear về cơ bản là phép nhân ma trận đơn giản ....\n",
    "# Nhiều tên khác: Ở Keras, chúng tôi gọi là Dense. Trong TensorFlow, chúng tôi đã gọi là FullConnected\n",
    "\n",
    "#Keras rất cao cấp - không tốt cho nghiên cứu / phát triển (chủ yếu cho giáo dục ...)\n",
    "#TensorFlow do Google phát triển - nó khá tốt\n",
    "\n",
    "# cho mô hình rất lớn, phức tạp, hiệu suất cao - TensorFlow tốt hơn / được tối ưu hóa nhiều\n",
    "     # chúng ở cấp thấp hơn PyTorch\n",
    "# đối với hầu hết mọi mô hình mà chúng tôi sử dụng (ngay cả trong nghiên cứu) - PyTorch tốt hơn nhiều\n",
    "# do đồ thị tính toán của nó .....\n",
    "\n",
    "#TensorFlow hỗ trợ một thứ gọi là TensorFlowLite, đó là cách\n",
    "# bạn muốn sử dụng cho điện thoại di động ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wonder whether have one extra layer, can reduce the loss!\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(3, 10),\n",
    "#     nn.Linear(10, 2)\n",
    "# )  #this is fine, but this is not the best practice!!\n",
    "   #because in the future, there are many layers and complex stuffs in neural network...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class là phương pháp hoàn hảo và tốt nhất để tạo mạng nơ-ron thuộc bất kỳ loại nào ...\n",
    "\n",
    "#format:\n",
    "'''\n",
    "class AnyNameCapitalized(nn.Module): #it must inherit nn.Module\n",
    "    def __init__():\n",
    "        super().__init__()  #super is basically inheriting nn.Module init\n",
    "        #we define all the layers here.....\n",
    "        \n",
    "    def forward(self, x):   #YOU CANNOT CHANGE THIS NAME, it MUST BE \"forward\"\n",
    "        x = layer1()\n",
    "        x = layer2()\n",
    "        return x\n",
    "'''\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(  3  ,  10   ,  2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.weight\n",
    "#model.fc1.weight\n",
    "#model.fc2.weight\n",
    "# model.fc1.weight #by default, these weights are uniformly close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.weight.shape  #this one is basically in the shape (out_features, in_feature)\n",
    "\n",
    "#you can imagine X @ W^T\n",
    "#after you transpose W, W^T becomes [3, 2]\n",
    "#which now you can do X @ W^T which is (anything, 3) @ (3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.bias  #why two bias???\n",
    "# model.fc1.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.parameters())  #this will list all the parameters (it's a object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p.numel() just flatten everything....\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#why 8 here??? - 6 weights and 2 bias....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=3, out_features=10, bias=True)\n",
       "  (fc2): Linear(in_features=10, out_features=2, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so how do we use our model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so you can perform a forward pass, simply using \n",
    "#format: model(inputs)\n",
    "\n",
    "# print(\"Inputs: \", inputs.shape)\n",
    "\n",
    "# output = model(inputs)  #(15, 3) @ (3, 2) = (15, 2)\n",
    "# print(output.shape)  #why output.shape is 15, 2??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#under the nn module, there are many loss function\n",
    "J_fn = nn.MSELoss()\n",
    "\n",
    "#later on, you will know how to use this....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 5  #it depends....trial and error....\n",
    "# #for num_epochs\n",
    "# for epoch in range(num_epochs):\n",
    "#     #for dataloader\n",
    "#     for x, y in dl:  \n",
    "#         #x: (batch, features) = (3, 3) \n",
    "#         #y: (batch, target)   = (3, 2)\n",
    "        \n",
    "#         #optional: you can put your x and y inside the CUDA (GPU) for speed up\n",
    "#         x.to(device)  #device is either cpu or cuda\n",
    "#         y.to(device)\n",
    "\n",
    "#         #1. predict (forward pass)\n",
    "#         yhat = model(x)\n",
    "        \n",
    "#         #2. calculate loss\n",
    "#         #sklearn.metrics.mse(y, yhat)\n",
    "#         #format: J_fn(inputs, targets)\n",
    "#         loss = J_fn(yhat, y)\n",
    "        \n",
    "#         #3. calculate gradient\n",
    "#         #3.1 clear out the previous gradients\n",
    "#         #format: optimizer.zero_grad()\n",
    "#         optim.zero_grad()\n",
    "        \n",
    "#         #3.2 called backward() on loss to retrieve all the gradients (backpropagation/backward pass)\n",
    "#         loss.backward()  #why called backward on loss!?\n",
    "#         #backward DOES NOT adjust the weight YET....just backpropagation\n",
    "#         #we want to calculate the gradients of all parameters (8 - 6 weights and 2 bias)\n",
    "#         #IN RESPECT TO THE LOSS....dJ/dw11,  dJ/dw12, dJ/dw13....., dJ/db1, dJ/db2\n",
    "        \n",
    "#         #4. update the parameters using the optim\n",
    "#         #W = W - alpha * gradient  #we don't need to do this here.....\n",
    "#         optim.step()  #optim already has learning rate, it also know all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Loss: 8590.18359375\n",
      "Epoch: 0 - Loss: 4780.64990234375\n",
      "Epoch: 0 - Loss: 6161.79052734375\n",
      "Epoch: 0 - Loss: 7275.24072265625\n",
      "Epoch: 0 - Loss: 10736.9033203125\n",
      "Epoch: 1 - Loss: 6945.66357421875\n",
      "Epoch: 1 - Loss: 11005.8583984375\n",
      "Epoch: 1 - Loss: 4419.08544921875\n",
      "Epoch: 1 - Loss: 13357.8876953125\n",
      "Epoch: 1 - Loss: 5743.67138671875\n",
      "Epoch: 2 - Loss: 4717.71728515625\n",
      "Epoch: 2 - Loss: 6766.87548828125\n",
      "Epoch: 2 - Loss: 6918.33154296875\n",
      "Epoch: 2 - Loss: 10840.4140625\n",
      "Epoch: 2 - Loss: 12169.3310546875\n",
      "Epoch: 3 - Loss: 10718.0458984375\n",
      "Epoch: 3 - Loss: 8208.2412109375\n",
      "Epoch: 3 - Loss: 4403.826171875\n",
      "Epoch: 3 - Loss: 9689.513671875\n",
      "Epoch: 3 - Loss: 8357.4951171875\n",
      "Epoch: 4 - Loss: 7179.408203125\n",
      "Epoch: 4 - Loss: 6906.91552734375\n",
      "Epoch: 4 - Loss: 12154.45703125\n",
      "Epoch: 4 - Loss: 6750.36669921875\n",
      "Epoch: 4 - Loss: 8350.578125\n",
      "Epoch: 5 - Loss: 9679.0068359375\n",
      "Epoch: 5 - Loss: 8347.7685546875\n",
      "Epoch: 5 - Loss: 7170.267578125\n",
      "Epoch: 5 - Loss: 7919.97509765625\n",
      "Epoch: 5 - Loss: 8189.26708984375\n",
      "Epoch: 6 - Loss: 8187.70703125\n",
      "Epoch: 6 - Loss: 5718.693359375\n",
      "Epoch: 6 - Loss: 9360.6982421875\n",
      "Epoch: 6 - Loss: 9667.119140625\n",
      "Epoch: 6 - Loss: 8336.7646484375\n",
      "Epoch: 7 - Loss: 6889.68017578125\n",
      "Epoch: 7 - Loss: 5713.20751953125\n",
      "Epoch: 7 - Loss: 9623.2783203125\n",
      "Epoch: 7 - Loss: 9351.3369140625\n",
      "Epoch: 7 - Loss: 9658.2138671875\n",
      "Epoch: 8 - Loss: 8172.87548828125\n",
      "Epoch: 8 - Loss: 9346.3017578125\n",
      "Epoch: 8 - Loss: 4378.16259765625\n",
      "Epoch: 8 - Loss: 9652.341796875\n",
      "Epoch: 8 - Loss: 9650.7705078125\n",
      "Epoch: 9 - Loss: 5702.81689453125\n",
      "Epoch: 9 - Loss: 10666.345703125\n",
      "Epoch: 9 - Loss: 8318.88671875\n",
      "Epoch: 9 - Loss: 8161.07177734375\n",
      "Epoch: 9 - Loss: 8316.0947265625\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10 \n",
    "for epoch in range(num_epochs):\n",
    "    for x, y in dl:  \n",
    "        x.to(device)  #device is either cpu or cuda\n",
    "        y.to(device)\n",
    "\n",
    "        yhat = model(x)\n",
    "        loss = J_fn(yhat, y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()  \n",
    "        optim.step()  \n",
    "        \n",
    "        print(f\"Epoch: {epoch} - Loss: {loss}\")\n",
    "        \n",
    "        #can you guys help tell what is the best hidden size?\n",
    "        #final exam is on Nov 22 9 - 12\n",
    "            #signal processing\n",
    "            #deep learning - pytorch\n",
    "        #project will be decided by Mr. Akraradet\n",
    "        #8 classes......14 lectures....\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[73., 67., 43.],\n",
       "         [91., 88., 64.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6109.3696, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#please create two numpy array of \n",
    "# [74, 68, 42], [92, 88, 65]\n",
    "x = np.array([[74, 68, 42], [92, 88, 65]], dtype='float32')\n",
    "#float  means 32 bits\n",
    "#double means 64 bits\n",
    "\n",
    "#please make it a tensor\n",
    "x_tensor = torch.tensor(x)\n",
    "\n",
    "#then use our model to predict the number of oranges and apples\n",
    "yhat = model(x_tensor)\n",
    "yhat\n",
    "\n",
    "#print the loss comparing with ds[0] and ds[1] - look at the y part ok...\n",
    "ytest = ds[0:2][1]\n",
    "loss = J_fn(yhat, ytest)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
